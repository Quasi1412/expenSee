<h2>Spend Analysis ETL Pipeline</h2>
This project was an attempt to replicate my knowledge from DE Zoomcamp by DataTalkClubs. The current pipeline extracts transactions from your downloaded statements, transforms them and loads it into a postgreDB for visulisation with any BI tools, here I will be using Metabase, which could help in analysing your expense and budget accordingly.  <br><br>


<h3>ðŸš€ Features</h3>
- Extract
    <ul>
    <li>Parse statements from multiple cards (CSV & PDF) using pandas for csv and leveraging Amazon Bedrock for pdf's.</li>
    <li>Normalize formats into a consistent transaction structure.</li>
    <li>Store intermediate outputs as CSVs.</li>
    </ul>
- Transform
  <ul>
    <li>Clean and normalize data across cards.</li>
    <li>Categorize transactions (Groceries, Restaurants, Insurance, Loans, Gas, Entertainment)using LLM-powered categorization (Ollama).</li>
  </ul>
- Load
  <ul>
    <li>Push final transactions into PostgreSQL.</li>
    <li>Ready for visualization in tools like Metabase.</li>
  </ul>
- Orchestration
  <ul>
    <li>Managed by Airflow DAGs with task separation for Extract â†’ Transform â†’ Load.</li>
    <li>Uses XComs to pass file paths and metadata between tasks.</li>
  </ul>  

  ```bash
  spend_analysis/
â”œâ”€â”€ airflow/               # Airflow project folder
â”‚   â”œâ”€â”€ dags/              # DAG definitions
â”‚   â”œâ”€â”€ config/            # (gitignored) airflow configs
â”‚   â”œâ”€â”€ plugins/           # custom plugins if any
â”‚   â”œâ”€â”€ logs/              # (gitignored) airflow logs
â”‚   â”œâ”€â”€ docker-compose.yaml
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ .env               # environment variables (not committed)
â”‚
â”œâ”€â”€ extract/               # Data extraction layer
â”‚   â”œâ”€â”€ bedrock_pdf_to_csv.py
â”‚   â”œâ”€â”€ fetch_data.py
â”‚   â””â”€â”€ process_statements.py
â”‚
â”œâ”€â”€ transform/             # Data transformation layer
â”‚   â”œâ”€â”€ normalise_data.py
â”‚   â””â”€â”€ categorise_with_ollama.py
â”‚
â”œâ”€â”€ load/                  # Database loading layer
â”‚   â””â”€â”€ push-data-to-db.py
â”‚
â”œâ”€â”€ statements/            # Input folder (local statements)
â”‚
â”œâ”€â”€ docker-compose.yaml    # Root-level compose file
â”œâ”€â”€ Dockerfile             # Root-level Dockerfile
â””â”€â”€ README.md


  ```

NOTE: Areas for improvement<br> 
    <ul>
        <li> Need to fix the pdf extraction logic. Using Bedrock seems to work but not very efficient and inconsistent. Eploring camelot package to find a general one-stop solution of all pdf structures </li>
        <li> Implement logic to detect and label recurring charges </li>
        <li> The logic to normalise date is hardcoded to the current year. Need to find a way to extract year from the pdf statement</li>


        
