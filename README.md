<h2>Spend Analysis ETL Pipeline</h2>
This project was an attempt to replicate my knowledge from DE Zoomcamp by DataTalkClubs. The current pipeline extracts transactions from your downloaded statements, transforms them and loads it into a postgreDB for visulisation with any BI tools, here I will be using Metabase, which could help in analysing your expense and budget accordingly.  <br><br>


<h3>ðŸš€ Features</h3>
- Extract
    <ul>
    <li>Parse statements from multiple cards (CSV & PDF).</li>
    <li>Normalize formats into a consistent transaction structure.</li>
    <li>Store intermediate outputs as CSVs.</li>
    </ul>
- Transform
  <ul>
    <li>Clean and normalize data across cards.</li>
    <li>Categorize transactions (Groceries, Restaurants, Insurance, Loans, Gas, Entertainment).</li>
    <li>Detect recurring charges using LLM-powered categorization (Ollama).</li>
  </ul>
- Load
  <ul>
    <li>Push final transactions into PostgreSQL.</li>
    <li>Ready for visualization in tools like Metabase.</li>
  </ul>
- Orchestration
  <ul>
    <li>Managed by Airflow DAGs with task separation for Extract â†’ Transform â†’ Load.</li>
    <li>Uses XComs to pass file paths and metadata between tasks.</li>
  </ul>  

  ```bash
  spend_analysis/
â”œâ”€â”€ airflow/               # Airflow project folder
â”‚   â”œâ”€â”€ dags/              # DAG definitions
â”‚   â”œâ”€â”€ config/            # (gitignored) airflow configs
â”‚   â”œâ”€â”€ plugins/           # custom plugins if any
â”‚   â”œâ”€â”€ logs/              # (gitignored) airflow logs
â”‚   â”œâ”€â”€ docker-compose.yaml
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ .env               # environment variables (not committed)
â”‚
â”œâ”€â”€ extract/               # Data extraction layer
â”‚   â”œâ”€â”€ bedrock_pdf_to_csv.py
â”‚   â”œâ”€â”€ fetch_data.py
â”‚   â””â”€â”€ process_statements.py
â”‚
â”œâ”€â”€ transform/             # Data transformation layer
â”‚   â”œâ”€â”€ normalise_data.py
â”‚   â””â”€â”€ categorise_with_ollama.py
â”‚
â”œâ”€â”€ load/                  # Database loading layer
â”‚   â””â”€â”€ push-data-to-db.py
â”‚
â”œâ”€â”€ statements/            # Input folder (local statements)
â”‚
â”œâ”€â”€ docker-compose.yaml    # Root-level compose file
â”œâ”€â”€ Dockerfile             # Root-level Dockerfile
â””â”€â”€ README.md


  ```
